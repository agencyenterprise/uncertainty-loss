{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty Learning with MNIST\n",
    "\n",
    "In this notebook we use the `uncertainty-loss` package to train some models on the MNIST dataset.  We then compare this to training with traditional cross entropy loss to see how the model perform in terms of accuracy and uncertainty quantification both for in-distribution and out-of-distribution data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mike/ae/alignment/uncertainty-loss/.venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version: 3.9.1 (default, Dec 11 2020, 06:28:49) \n",
      "[Clang 10.0.0 ]\n",
      "torch version: 1.12.1\n",
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import sys \n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "import os \n",
    "\n",
    "# force reproduciblity\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "import torch\n",
    "from torch import nn \n",
    "from torch.nn import functional as F\n",
    "import torchvision \n",
    "import torchvision.transforms.functional as TF\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.ticker as mtick\n",
    "import numpy as np\n",
    "\n",
    "import uncertainty_loss as ul\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# force reproduciblity\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "print('python version:', sys.version)\n",
    "print('torch version:', torch.__version__)\n",
    "print('device:', DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# Load the MNIST data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = torchvision.datasets.MNIST(\"./data/\", download=True, transform=torchvision.transforms.ToTensor())\n",
    "mnist_test = torchvision.datasets.MNIST(\"./data/\", train=False, download=True, transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_examples(examples:list, grid=5):\n",
    "    \"\"\"Plots a list of images together with their label.\"\"\"\n",
    "    fig, axes = plt.subplots(grid,grid, figsize=(4,4))\n",
    "    for i,ax in enumerate(axes.ravel()):\n",
    "        ax.imshow(examples[i][0][0])\n",
    "        ax.set_xlabel(examples[i][1], fontsize=10)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    fig.tight_layout()\n",
    "    fig.set_facecolor('white')\n",
    "    return fig, ax \n",
    "\n",
    "fig,ax = plot_examples([mnist_train[i] for i in range(25)], grid=5)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "# Set up model\n",
    "\n",
    "We use a simple LeNet CNN to train against the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(torch.nn.Module):\n",
    "    r\"\"\"LeNet convoluational architecture.\"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size=5, d_hidden=500, activation=F.relu, dropout=.3, classifier_activation=None):\n",
    "        \"\"\"Initialize a LeNet instance.\n",
    "        \n",
    "        Args:\n",
    "            filter_size: \n",
    "            activation:\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, kernel_size=kernel_size)\n",
    "        self.conv2 = nn.Conv2d(20,50, kernel_size=kernel_size)\n",
    "        self.max_pool = nn.MaxPool2d(1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(20000, d_hidden)\n",
    "        self.fc2 = nn.Linear(d_hidden, 10)\n",
    "        self.activation=activation\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier_activation = classifier_activation\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        \"\"\"Run a forward pass of the model.\"\"\"\n",
    "        x = self.activation(self.max_pool(self.conv1(x)))\n",
    "        x = self.activation(self.max_pool(self.conv2(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(self.activation(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        if self.classifier_activation is not None:\n",
    "            x = self.classifier_activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# Training code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation helpers\n",
    "\n",
    "We want to evaluate the uncertainy of correct and incorrect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncertainty_accuracy(evidence, target):\n",
    "    \"\"\"Calculates the uncertainty of accurate and inaccurate classifications.\n",
    "    \n",
    "    Args:\n",
    "        evidence: evidence tensor of the form :math:`g(model(x))` where \n",
    "            :math:`g` is any non-negative transformation.\n",
    "        target: the true class labels in sparse or one-hot format \n",
    "    \"\"\"\n",
    "    u = uncert.uncertainty(evidence)\n",
    "    y_pred = torch.argmax(evidence, -1)\n",
    "    if target.dim() == 2:\n",
    "        target = torch.argmax(target, -1) # convert one-hot to sparse\n",
    "    hit_uncert = torch.mean(torch.tensor([u[i] for i in range(len(evidence)) if y_pred[i]==target[i]]))\n",
    "    miss_uncert = torch.mean(torch.tensor([u[i] for i in range(len(evidence)) if y_pred[i] != target[i]]))\n",
    "    return hit_uncert, miss_uncert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_loop(*, model, opt, train_dl,  val_dl=None, epochs=50, reg_steps=10000):\n",
    "    r\"\"\"Runs a model training loop\"\"\"\n",
    "    metrics = defaultdict(list)\n",
    "    reg_factor = 0.0\n",
    "    reg_steps_size = 1/reg_steps\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for i, (x,y) in enumerate(train_dl):\n",
    "            opt.zero_grad()\n",
    "            evidence = model(x.to(DEVICE)).cpu()\n",
    "            loss = uncert.evidential_uncertainty_loss(evidence, y, reg_factor)\n",
    "            reg_factor += reg_steps_size\n",
    "            reg_factor = min(reg_factor, 1.0)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            y_pred = torch.argmax(evidence, -1)\n",
    "            target = y if y.dim()==1 else torch.argmax(y, -1)\n",
    "            train_hits = y_pred==target\n",
    "            metrics['train_acc'].append((sum(train_hits)/len(train_hits)).item())\n",
    "            metrics['train_step'].append(i)\n",
    "            metrics['train_step_in_epoch'].append(epoch)\n",
    "            metrics['train_loss'].append(loss.item())\n",
    "\n",
    "            print(f\"Epoch {epoch}: train loss {loss:.3f} reg factor {reg_factor:.3f}\", end=\"\\r\")\n",
    "\n",
    "        if val_dl is not None:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                pairs = [(model(x.to(DEVICE)).cpu(),y) for x,y in val_dl]\n",
    "                val_loss = torch.nanmean(torch.tensor([uncert.evidential_uncertainty_loss(x,y,reg_factor) for x,y in pairs]))\n",
    "                # measure accuracy\n",
    "                y_pred = torch.stack([torch.argmax(p[0],-1) for p in pairs], 0).reshape(-1)\n",
    "                if pairs[0][1].dim()==2:\n",
    "                    target = torch.tensor([torch.argmax(p[1]) for p in pairs])\n",
    "                else:\n",
    "                    target = torch.stack([p[1] for p in pairs], 0).reshape(-1)\n",
    "                hits = y_pred==target \n",
    "\n",
    "                val_acc = sum(hits)/len(hits)\n",
    "                metrics['val_accuracy'].append(val_acc)\n",
    "\n",
    "                # meausre a bunch of other stuff\n",
    "                u_acc = [uncertainty_accuracy(x, y) for x,y in pairs]\n",
    "                hit_uncert = torch.nanmean(torch.tensor([u[0] for u in u_acc]))\n",
    "                miss_uncert = torch.nanmean(torch.tensor([u[1] for u in u_acc]))\n",
    "                metrics['val_loss'].append(val_loss.item())\n",
    "                metrics['hit_uncertainty'].append(hit_uncert)\n",
    "                metrics['miss_uncertainty'].append(miss_uncert)\n",
    "                metrics['epoch'].append(epoch)\n",
    "                metrics['val_step'].append(i)\n",
    "\n",
    "        if epoch %10 == 0:        \n",
    "            print(f'\\nEpoch {epoch}: train loss {loss:.3f} val_acc: {val_acc:.3f} val_loss {val_loss:.3f} hit u {hit_uncert:.3f} miss u {miss_uncert:.3f}')\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train models\n",
    "We train a model with each type of evidence: relu, exp, softplus.  \n",
    "\n",
    "From reading the [paper's original notebook](https://muratsensoy.github.io/uncertainty.html) they suggest `exp` evidence works the best.  However, in the notebook the rotating image classifier with the \"cleanest\" curve is the one trained with `relu` evidence.  We train all 3 at once so we can experiment with them later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_funcs = [uncert.exp_evidence, uncert.relu_evidence, uncert.softplus_evidence]\n",
    "e_names = ['exp', 'relu', 'softplus']\n",
    "train_dl = torch.utils.data.DataLoader(mnist_train, batch_size=1000, num_workers=8)\n",
    "val_dl = torch.utils.data.DataLoader(mnist_test, batch_size=32, drop_last=True)\n",
    "reg_steps = 10*len(train_dl) # saturate after 10 epochs\n",
    "metrics = {}\n",
    "models = {}\n",
    "\n",
    "model_path = pathlib.Path('./artifacts/models')\n",
    "model_path.mkdir(parents=True, exist_ok=True)\n",
    "metric_path = model_path / 'training_metrics'\n",
    "metric_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for evidence_func, name in zip(e_funcs, e_names):\n",
    "    torch.manual_seed(SEED)\n",
    "    model_file = model_path / f'mnist_evidential_{name}.pt'\n",
    "    metrics_file = metric_path / f'mnist_evidential_{name}.pkl'\n",
    "    if model_file.exists() and metrics_file.exists():\n",
    "        state_dict = torch.load(model_file)\n",
    "        _model = LeNet()\n",
    "        _model.load_state_dict(state_dict)\n",
    "        _model.to(DEVICE)\n",
    "        with open(metrics_file, 'r') as f:\n",
    "            _metrics = pickle.load(f)\n",
    "        models[name]= _model \n",
    "        metrics[name] = _metrics\n",
    "    else:\n",
    "        _model = LeNet(classifier_activation=evidence_func).to(DEVICE)\n",
    "        _metrics = fit_loop(model=_model, opt=torch.optim.AdamW(_model.parameters(), lr=1e-3), train_dl=train_dl, val_dl=val_dl, reg_steps=reg_steps)\n",
    "\n",
    "        torch.save(_model.state_dict(), model_file)\n",
    "        with open(metrics_file, 'wb') as f:\n",
    "            pickle.dump(_metrics, f)\n",
    "        models[name] = _model \n",
    "        metrics[name] = _metrics \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "# Evaluate the model\n",
    "\n",
    "We now look at some of the metrics during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_validation_metrics(metrics, ax=None):\n",
    "    \"\"\"Plots the uncertainty and accuracy metrics.\"\"\"\n",
    "    import matplotlib.ticker as mtick\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1,1, figsize=(5,5))\n",
    "    ax.plot(metrics['epoch'], metrics['hit_uncertainty'], '-o', label='hit uncertainty')\n",
    "    ax.plot(metrics['epoch'], metrics['miss_uncertainty'], '-x', label='miss_uncertainty')\n",
    "    ax.set_xlabel('epochs')\n",
    "    ax.set_title('Validation Uncertainty')\n",
    "    ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "    return ax \n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(10,5), sharey=True)\n",
    "for i, (name, m) in enumerate(metrics.items()):\n",
    "    plot_validation_metrics(m, ax=ax[i])\n",
    "    ax[i].set_title(name)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots show that the uncertainty for examples that the model got correct is much lower than the uncertainty for examples it gets incorrect.  That is what we anticipated.\n",
    "\n",
    "It seems the exponential evidence has the lowest uncertainty for hits, the relu has the highest uncertainty for misses, and the softplus is kinda of the worst of both worlds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_results(model, ds):\n",
    "    \"\"\"Computes predictions and uncertainty for each example in the dataset.\"\"\"\n",
    "    model.eval()\n",
    "    image = []\n",
    "    label = []\n",
    "    prediction = [] \n",
    "    uncert_ = []\n",
    "    device = next(model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        for x,y in ds:\n",
    "            image.append(x.numpy())\n",
    "            label.append(y)\n",
    "            _evidence = model(x.unsqueeze(0).to(device)).cpu()\n",
    "            _pred = torch.argmax(_evidence, -1).item()\n",
    "            prediction.append(_pred)\n",
    "            uncert_.append(uncert.uncertainty(_evidence).item())\n",
    "    return image, label, prediction, uncert_\n",
    "\n",
    "\n",
    "result = {}\n",
    "for name, m in models.items():\n",
    "    result[name] = get_validation_results(m, mnist_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at some examples with high uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_uncertainty(r, reverse=False):\n",
    "    \"\"\"Sorts the results by uncertainty.\n",
    "    \n",
    "    Probably a smarter way to do this, but it works for now.\n",
    "    \"\"\"\n",
    "    r = sorted([t for t in zip(*r)], key=lambda x: x[-1], reverse=reverse)\n",
    "    return tuple(zip(*r))\n",
    "\n",
    "\n",
    "def plot_uncertain_examples(images, labels, predictions, uncertainty, grid=3):\n",
    "    \"\"\"Plots images some meta data\"\"\"\n",
    "    fig, axes = plt.subplots(grid, grid, figsize=(6,6))\n",
    "    for i, ax in enumerate(axes.ravel()):\n",
    "        ax.imshow(images[i][0])\n",
    "        ax.set_xlabel(f\"{labels[i]}, {predictions[i]}/{uncertainty[i]:.1f}\")\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    fig.tight_layout()\n",
    "    return fig, ax    \n",
    "\n",
    "def select_n(result, n):\n",
    "    \"\"\"Selects the first n examples from the results.\"\"\"\n",
    "    return tuple([r[:n] for r in result])\n",
    "\n",
    "\n",
    "result = {name: sort_by_uncertainty(r, reverse=True) for name, r in result.items()}\n",
    "fig,ax = plot_uncertain_examples(*select_n(result['exp'],36), grid=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of these examples are \"difficult\" for example the 5 in the last row.  It is also interesting to see that some extra \"ink\" in the 1 and 0 in the second row cause the model to become less certain even when the image itself is very clear that its a one.  Not that we care for mnist, but this type of analysis would lead us to conlude that \"random ink\" could be a good augmentation strategy.  This is an example of the type of thing we might do with uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "#### Filter by uncertainty scores\n",
    "We can simulate what would happen if we flagged elements with high uncertainty for human review.  In other words, we score each example for uncertainty and then we filter out those above a certain threshold and measure the accuracy on the ones we are certain about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_vs_uncertainty(result, u_range=None, ax=None):\n",
    "    images, targets, pred, uncert_ = sort_by_uncertainty(result)\n",
    "    hits = np.array(targets) == np.array(pred)\n",
    "    acc = []\n",
    "    uncert_vals = []\n",
    "    bars = []\n",
    "    u_range = np.linspace(0,1,50) if u_range is None else u_range\n",
    "    for u in u_range:\n",
    "        u_index = np.searchsorted(uncert_, u)\n",
    "        if len(hits[:u_index])>0:\n",
    "            acc.append(sum(hits[:u_index])/len(hits[:u_index]))\n",
    "            uncert_vals.append(u)\n",
    "            bars.append(len(hits[:u_index])/len(hits))\n",
    "    plt.style.use('seaborn-whitegrid')\n",
    "    if ax is None:\n",
    "        fig,ax = plt.subplots(figsize=(4,4))\n",
    "    ax.plot(uncert_vals, acc, '-o', label='accuracy')\n",
    "    ax.plot(uncert_vals, bars, '-x', color='orange', label='percent examples')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('uncertainty')\n",
    "    ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(1,3, figsize=(10,5), sharey=True)\n",
    "for i,name in enumerate(result):\n",
    "    plot_acc_vs_uncertainty(result[name], ax=ax[i])\n",
    "    ax[i].set_title(name);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually a bit hard to interpret.  In the case where the uncertainty threshold is very very low, its clear that exp is doing the best (the most samples are making through the threshold with high accuracy).  Let's compare them directly.\n",
    "\n",
    "We compare them by looking over a range of uncertainty thresholds.  For each threshold we score the examples that fall below that threshold, and the remaining are \"sent for review\".  Then to compare we ask \"for a fixed number of examples going to review (say 5%) what is the accuracy of the model on the remaining examples (95%)\".  This lets us compare model accuracy against an absolute number which measure the degree to which we can automate the model vs how much we need a human in the loop to achieve high accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_results(*args,labels=None, u_range=None, ax=None):\n",
    "    labels = [None]*len(args) if labels is None else labels\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1,1, figsize=(5,5))\n",
    "    \n",
    "    max_review = -1\n",
    "    min_review = 10000\n",
    "    u_range = np.linspace(0,1,50) if u_range is None else u_range\n",
    "    for i, result in enumerate(args):    \n",
    "        images, targets, pred, uncert_ = sort_by_uncertainty(result)\n",
    "        hits = np.array(targets) == np.array(pred)\n",
    "        acc = []\n",
    "        uncert_vals = []\n",
    "        percent_review = []\n",
    "\n",
    "        for u in u_range:\n",
    "            u_index = np.searchsorted(uncert_, u,side='right')\n",
    "            if len(hits[:u_index])>0:\n",
    "                acc.append(sum(hits[:u_index])/len(hits[:u_index]))\n",
    "                uncert_vals.append(u)\n",
    "                percent_review.append(1-(len(hits[:u_index])/len(hits)))\n",
    "        plt.style.use('seaborn-whitegrid')\n",
    "        ax.plot(percent_review, acc, '-o', label=labels[i])\n",
    "        #plt.plot(uncert_vals, bars, '-x', color='orange', label='percent examples')\n",
    "        plt.legend()\n",
    "        ax.set_xlabel('percent reviewed')\n",
    "        ax.set_ylabel('accuracy')\n",
    "        ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "        ax.xaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "        max_review = max(max_review, max(percent_review))\n",
    "        min_review = min(min_review, min(percent_review))\n",
    "    ax.set_xlim([min_review-.02, max_review+.02])\n",
    "\n",
    "compare_results(*result.values(), labels=list(result.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also hard to interpret so we will zoom in below, but one interesting thing we can learn from this is that if we need 100% accuracy and are willing to review a high percentage of examples (20%) then we have to use `relu` or `softplus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_results(*result.values(), labels=list(result.keys()), u_range=np.linspace(0,.1,50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows the examples whose uncertainty scores is less than or equal to 0.1, the \"highly certain examples\".  We can see that exponential evidence has more highly certain examples (smaller percent reviewed) but that the accuracy of these examples is only so high.  If we were willing or needing to achieve high accuracy, at the cost of more review, we would need to use relu or softplus, both able to achieve 100% accuracy, softplus doing so with ~17% review, meaning the model can achieve 100% accuracy on the 83% it is most confident about, leaving the remaining 17% for human review.\n",
    "\n",
    "The choice of activation will likely be depending on the application.  If we need more automation, then `exp` seems like a good choice since it achieves the highest out of the box accuracy and can achieve even higher accuracy with a small percentage sent to review.  If we need extremely high accuracy and are willing to sacrifice automation then `softplus` or `relu` seem like better choices since they can achieve 100% accuracy at the cost of more review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Manipulation\n",
    "\n",
    "We now see how well these methods work for OOD samples.  We can create fake OOD samples by rotating images.  The model was not trained with any data augmentation so has never seen a \"sideways 2\".  The uncertainty for these rotated images should be high. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_uncertainty_rotated(model, image, ax=None):\n",
    "    \"\"\"Plots the uncertainty for a given image over a range of rotations.\"\"\"\n",
    "    if ax is None:\n",
    "        fig,ax = plt.subplots(1,1, figsize=(5,5))\n",
    "    device = next(model.parameters()).device\n",
    "    image = image.to(device)\n",
    "    uncert_ = []\n",
    "    angles = np.linspace(0,180,25)\n",
    "    for a in angles:\n",
    "        rotated = TF.rotate(image, a)\n",
    "        evidence = model(rotated.unsqueeze(0)).cpu()\n",
    "        uncert_.append(uncert.uncertainty(evidence).item())\n",
    "    ax.plot(angles, uncert_, '-o')\n",
    "    ax.set_xlabel('rotation (degrees)')\n",
    "    ax.set_ylabel('uncertainty')\n",
    "    \n",
    "def get_digits(ds, digit):\n",
    "    \"\"\"Gets all examples of a given digit from the dataset.\"\"\"\n",
    "    return [x for x,y in ds if y==digit]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = get_digits(mnist_test, 1)\n",
    "one_example = ones[1]\n",
    "plt.style.use('default')\n",
    "plt.imshow(one_example[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,1, figsize=(5,5), sharex=True)\n",
    "for i, (name,model) in enumerate(models.items()):\n",
    "    plot_uncertainty_rotated(models[name], one_example, ax=ax[i])\n",
    "    ax[i].set_title(name)\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UPDATE: After re-running this a few times (retraining) it seems like generally the softplus and relu are the most confident and exp tends to \"make more mistakes\" when rotating.  It's not clear that there is a winner here - but generally speaking it seems like `exp` evidence is more accurate but `relu` and `softplus` seem to be better at quantifying uncertainty.\n",
    "\n",
    "Note to self: What would happen if we chose a hybrid evidence function, e.g. the average of the three?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Try another example\n",
    "\n",
    "The example below is particularly confusing for the models when rotated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,1, figsize=(5,5), sharex=True)\n",
    "for i, (name,model) in enumerate(models.items()):\n",
    "    plot_uncertainty_rotated(models[name], ones[13], ax=ax[i])\n",
    "    ax[i].set_title(name)\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2, figsize=(5,3))\n",
    "ax[0].imshow(ones[13][0])\n",
    "rot_img = TF.rotate(ones[13], 90)\n",
    "ax[1].imshow(rot_img[0])\n",
    "_evidence = models['relu'](rot_img.unsqueeze(0).to(DEVICE)).cpu()\n",
    "print(f\"predicted class: {torch.argmax(_evidence)}\")\n",
    "for a in ax:\n",
    "    a.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For whatever reason, this particular `1` looks like a `5` to the model when its rotated this way and the model is more certain about that compared to other examples of `1`s.  I'm not sure why, but it does show that this uncertainty loss is not 100% robust to OOD.\n",
    "\n",
    "It would be interesting to compare the results here to the setup in the paper: [Information Aware Max-Norm Dirichlet Networks for\n",
    "Predictive Uncertainty Estimation](https://arxiv.org/pdf/1910.04819.pdf)\n",
    "which builds on the idea of using dirichlet distributions to model uncertainty but whose math I have not fully gone through enough to implement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "# Compare with softmax\n",
    "\n",
    "It is reasonable to wonder if we could get the same effect by using traditional neural network training without uncertainty - instead can we just filter by model \"probabilities\"?  To answer this question we train another neural network with traditional cross entropy loss.  We then measure uncertainty as `1-max(softmax(logits))` which is basically the amount of probability not accounted for by the most likely class. If the model predicts digit 2 with 99% probability then the uncertainty is 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_accuracy(logits, target):\n",
    "    p = F.softmax(logits, -1)\n",
    "    y_pred = torch.argmax(logits, -1)\n",
    "    if target.dim() == 2:\n",
    "        target = torch.argmax(target, -1) # convert one-hot to sparse\n",
    "    hit_proba = torch.mean(torch.tensor([torch.max(p[i]) for i in range(len(p)) if y_pred[i]==target[i]]))\n",
    "    miss_proba = torch.mean(torch.tensor([torch.max(p[i]) for i in range(len(p)) if y_pred[i] != target[i]]))\n",
    "    return hit_proba, miss_proba\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cross_entropy_fit_loop(*, model, train_dl, val_dl, opt, epochs=50):\n",
    "    metrics = defaultdict(list)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for i, (x,y) in enumerate(train_dl):\n",
    "            opt.zero_grad()\n",
    "            logits = model(x.to(DEVICE)).cpu()\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            y_pred = torch.argmax(logits, -1)\n",
    "            target = y if y.dim()==1 else torch.argmax(y, -1)\n",
    "            train_hits = y_pred==target\n",
    "            metrics['train_acc'].append((sum(train_hits)/len(train_hits)).item())\n",
    "            metrics['train_step'].append(i)\n",
    "            metrics['train_step_in_epoch'].append(epoch)\n",
    "            metrics['train_loss'].append(loss.item())\n",
    "\n",
    "            print(f\"Epoch {epoch}: train loss {loss:.3f}\", end=\"\\r\")\n",
    "\n",
    "        if val_dl is not None:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                pairs = [(model(x.to(DEVICE)).cpu(),y) for x,y in val_dl]\n",
    "            val_loss = torch.nanmean(torch.tensor([F.cross_entropy(x,y) for x,y in pairs]))\n",
    "            \n",
    "            # measure accuracy\n",
    "            y_pred = torch.stack([torch.argmax(p[0],-1) for p in pairs], 0).reshape(-1)\n",
    "            if pairs[0][1].dim()==2:\n",
    "                target = torch.tensor([torch.argmax(p[1]) for p in pairs])\n",
    "            else:\n",
    "                target = torch.stack([p[1] for p in pairs], 0).reshape(-1)\n",
    "            hits = y_pred==target \n",
    "\n",
    "            val_acc = sum(hits)/len(hits)\n",
    "            metrics['val_accuracy'].append(val_acc)\n",
    "\n",
    "            # meausre a bunch of other stuff\n",
    "            metrics['val_loss'].append(val_loss.item())\n",
    "            metrics['epoch'].append(epoch)\n",
    "            metrics['val_step'].append(i)\n",
    "            s_acc = [softmax_accuracy(x, y) for x,y in pairs]\n",
    "            # naively say uncertainty is 1-proba\n",
    "            hit_uncert = 1 - torch.nanmean(torch.tensor([s[0] for s in s_acc]))\n",
    "            miss_uncert = 1 - torch.nanmean(torch.tensor([s[1] for s in s_acc]))\n",
    "            metrics['hit_uncertainty'].append(hit_uncert)\n",
    "            metrics['miss_uncertainty'].append(miss_uncert)\n",
    "            \n",
    "\n",
    "    \n",
    "            print(f'\\nEpoch {epoch}: train loss {loss:.3f} val_acc: {val_acc:.3f} val_loss {val_loss:.3f} hit u {hit_uncert:.3f} miss u {miss_uncert:.3f}')\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = model_path / 'mnist_cross_entropy.pt'\n",
    "metrics_file = model_path / 'training_metrics' / 'mnist_cross_entropy.pkl'\n",
    "\n",
    "\n",
    "if model_file.exists() and metrics_file.exists():\n",
    "    state_dict = torch.load(model_file)\n",
    "    model_ce = LeNet()\n",
    "    model_ce.load_state_dict(state_dict)\n",
    "    with open(metrics_file, 'rb') as f:\n",
    "        metrics_ce = pickle.load(f)\n",
    "else:\n",
    "    torch.manual_seed(SEED)\n",
    "    model_ce = LeNet(classifier_activation=None)\n",
    "    model_ce.to(DEVICE)\n",
    "    metrics_ce = cross_entropy_fit_loop(\n",
    "        model=model_ce, \n",
    "        train_dl=train_dl, \n",
    "        val_dl=val_dl, \n",
    "        opt=torch.optim.AdamW(model_ce.parameters(), lr=1e-3, weight_decay=5e-3),\n",
    "        epochs=50)\n",
    "    torch.save(model_ce.state_dict(), model_file)\n",
    "    with open(metrics_file, 'wb') as f:\n",
    "        pickle.dump(metrics_ce, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Crossentropy uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_validation_metrics(metrics_ce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the uncertainty for correct answers is much lower than that for incorrect answers, but looking at the previous plot from the model trained with dirichlet uncertainty loss, we can see this gap is much smaller and is getting smaller over training epochs, not larger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_results_ce(model, ds):\n",
    "    \"\"\"Computes predictions and uncertainty for each example in the dataset.\"\"\"\n",
    "    model.eval()\n",
    "    image = []\n",
    "    label = []\n",
    "    prediction = [] \n",
    "    uncert_ = []\n",
    "    device = next(model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        for x,y in ds:\n",
    "            image.append(x.numpy())\n",
    "            label.append(y)\n",
    "            _logits = model(x.unsqueeze(0).to(device)).cpu()\n",
    "            _pred = torch.argmax(_logits, -1).item()\n",
    "            prediction.append(_pred)\n",
    "            uncert_.append(1-torch.max(F.softmax(_logits,-1)).item())\n",
    "    return image, label, prediction, uncert_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ce = get_validation_results_ce(model_ce, mnist_test)\n",
    "result_ce = sort_by_uncertainty(result_ce, reverse=True)\n",
    "plt.style.use('default')\n",
    "plot_uncertain_examples(*select_n(result_ce, 36), grid=6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some overlap here and some differences with the examples found the the other uncertainty models.  Note that the uncertainty score maxes out at 0.6.\n",
    "\n",
    "\n",
    "### Plot Accuracy vs Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_vs_uncertainty(result_ce)\n",
    "plt.xlabel('softmax \"uncertainty\"');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like we can get up to 99.6% accuracy on all but the 1.4% of hardest examples using softmax uncertainty.  Let's see how this compares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Uncertainty vs Softmax Cross Entropy\n",
    "The plots are very hard to compare visually.  Let's plot them together so we can see which is more accurate.  We do this by holding out the top X% most uncertain examples that we would send for \"human review\".  The remaining 100-X% examples are sent to the model to be scored and the accuracy metrics is plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,1, figsize=(6,10), sharey=True, sharex=True)\n",
    "\n",
    "compare = [result['softplus'], result_ce]\n",
    "compare = [result['exp'], result['relu']]\n",
    "\n",
    "for i,(name, r) in enumerate(result.items()):\n",
    "    compare_results(r, result_ce, labels=None, ax=ax[i])\n",
    "    ax[i].set_title(name)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear that when we need more accuracy with more review, all the uncertainty methods are better.  Let's also zoom in on the portion of \"small amount of review\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,1, figsize=(6,10), sharey=True, sharex=True)\n",
    "\n",
    "compare = [result['softplus'], result_ce]\n",
    "compare = [result['exp'], result['relu']]\n",
    "\n",
    "for i,(name, r) in enumerate(result.items()):\n",
    "    compare_results(r, result_ce, labels=None, ax=ax[i], u_range=np.linspace(.3,1,10))\n",
    "    ax[i].set_title(name)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the exponential evidence is the best right out of the box without filtering.  The relu network doesn't shine until we get up to review about 1.5% and is about equivalent to `exp` in that range.  The softplus is better than softmax out of the box but not quite as good as relu or exp for very small amounts of review (even though we know its much better when we allow more of review, shooting up to 100% accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossentropy Image Rotation\n",
    "\n",
    "We have seen that the uncertain loss offers a better trade-off of accuracy and \"automation\" when filtering examples by sending highly uncertain examples for human review, compared to a model trained with traditional softmax cross entropy.  One could reasonably argue that this is due to randomness - the uncertainty model happened to do better in this particular case, with this particular initialization, etc.\n",
    "\n",
    "\n",
    "Let's also check that when we rotate images, we get better results from the uncertainty models than we do from the crossentropy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_uncertainty_rotated_ce(model, image, ax=None, classes=None):\n",
    "    \"\"\"Plots the uncertainty for a given image over a range of rotations.\"\"\"\n",
    "    if ax is None:\n",
    "        fig,ax = plt.subplots(1,1, figsize=(5,5))\n",
    "    device = next(model.parameters()).device\n",
    "    image = image.to(device)\n",
    "    uncert_ = []\n",
    "    probas = defaultdict(list)\n",
    "    angles = np.linspace(0,180,25)\n",
    "    for a in angles:\n",
    "        rotated = TF.rotate(image, a)\n",
    "        _logits = model(rotated.unsqueeze(0)).cpu()\n",
    "        _p = F.softmax(_logits,-1)\n",
    "        if classes is not None:\n",
    "            for c in classes:\n",
    "                probas[c].append(_p[0,c].detach().numpy())\n",
    "            \n",
    "        uncert_.append(1-torch.max(_p).item())\n",
    "\n",
    "\n",
    "    if classes is not None:\n",
    "        for c,p in probas.items():\n",
    "            ax.plot(angles, p, '-o', label=c)\n",
    "        ax.set_ylabel('softmax probablity')\n",
    "    else:\n",
    "        ax.plot(angles, uncert_, '-o', label='uncertainty')\n",
    "        ax.set_ylabel('uncertainty')\n",
    "    ax.set_xlabel('rotation (degrees)')\n",
    "\n",
    "\n",
    "plot_uncertainty_rotated_ce(model_ce, one_example, classes=[1,5,7])\n",
    "plt.title('softmax uncertainty')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example the model is sure its a 1, then it thinks its a 1 or a 5 (with a slight preference for 5) and finally it is very sure its a 7, before rotating back to being sure its a 1.  We can see that the softmax probabilities are far too optimistic, with probabilities of 100% certain the rotated 1 is a 7.  In contrast, the plots above shows that we can detect these OOD rotations by using uncertainty loss.\n",
    "\n",
    "See below for a direct comparison to the plots above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_uncertainty_rotated_ce(model_ce, one_example)\n",
    "plt.title('softmax uncertainty')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a another view of the above plot and compares to the uncertainty plots for rotation above.  It shows the model is highly certain (0 uncertainty) for many of the rotations, even though it should be more uncertain since its never seen a rotated 1 during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "# Conclusion\n",
    "In this notebook we trained several models with uncertainty loss and with traditional cross entropy loss.  We built more intuition for how well uncertainty works, at least on a toy dataset, and saved these models for future use. \n",
    "\n",
    "The next steps are \n",
    "\n",
    "1. Build an interactive web app for highlighting uncertainty, starting with mnist and widgets for rotation and other augmentations.\n",
    "2. Look for a more realistic dataset, ideally with high impact, e.g. medical data, financial data (loans), etc.\n",
    "    * ideally we could show that by gaining even small amounts of accuracy by sending the appropriate examples for human review, this could have an outsized impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "889fb307d39c28aaf69ee69ff96b3c90ba616c5b804828403814ea2d09dc55ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
